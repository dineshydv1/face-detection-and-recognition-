{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face detection Using OpenCV\n",
    "\n",
    "OpenCV deep learning face detector is based on Single-Shot_Detector (SSD) framework with a ResNet base network (unlike other OpenCV SSDs that you may have seen which typically use MobilNet as the base network). \n",
    "\n",
    "In first part we will see how to apply face detection to single input images. \n",
    "\n",
    "Then in next section, we will check how to modify this code and apply face detection to videos, video streams and webcams. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Face detection on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import require libraires\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input arguments\n",
    "# input image \n",
    "#arg_image = \".//DATA//images//rooster.jpg\"\n",
    "arg_image = \".//DATA//images//iron_chic.jpg\"\n",
    "# path to Caffe deploy prototxt file\n",
    "arg_prototxt = \".//MODELS//FaceDetection//deploy.prototxt\"\n",
    "# path to Caffe pre-trained model\n",
    "arg_model = \".//MODELS//FaceDetection//res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "# minimum probability to filter weak detections\n",
    "arg_confidence = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading face detection model...\n"
     ]
    }
   ],
   "source": [
    "# load the mode\n",
    "print(\"[INFO] Loading face detection model...\")\n",
    "net = cv2.dnn.readNetFromCaffe(arg_prototxt, arg_model)\n",
    "\n",
    "# load input image\n",
    "image = cv2.imread(arg_image)\n",
    "# capture its shape\n",
    "(h,w) = image.shape[:2]\n",
    "# convert image to blob to feed to net\n",
    "blob = cv2.dnn.blobFromImage( cv2.resize(image,(300,300)),1.0, (300,300), (104.0, 177.0, 123.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing face detection\n",
    "net.setInput(blob)\n",
    "# get predictions\n",
    "detections = net.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 200, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[0.        , 1.        , 0.9999083 , ..., 0.11946487,\n",
       "          0.86777306, 0.5585621 ],\n",
       "         [0.        , 1.        , 0.9985989 , ..., 0.25462836,\n",
       "          0.52284336, 0.6467794 ],\n",
       "         [0.        , 1.        , 0.99735343, ..., 0.30858952,\n",
       "          0.2786947 , 0.7022801 ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]]]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# detections is array of 200 element where each element is of 7 values\n",
    "# these 7 values are: \n",
    "# (x,class_label(face),confidence,startX,startY,endX,endY)\n",
    "# last 4 are BB coordinates\n",
    "print(detections.shape)\n",
    "detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets toop over detections and draw bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence:  0.9999083\n",
      "260 59 433 279\n",
      "confidence:  0.9985989\n",
      "135 127 261 323\n",
      "confidence:  0.99735343\n",
      "6 154 139 351\n"
     ]
    }
   ],
   "source": [
    "# loop over the detections\n",
    "for i in np.arange(0,detections.shape[2]):\n",
    "    # extract the confidence (i.e. probability)\n",
    "    # of finding a face\n",
    "    confidence = detections[0,0,i,2]\n",
    "    # filter out weak detections by checking confidence\n",
    "    # with minimum confidence\n",
    "    if(confidence > arg_confidence):\n",
    "        print(\"confidence: \",confidence)\n",
    "        # get BB cordinates\n",
    "        # compute the (x,y) cordinates of BB of the object\n",
    "        box = detections[0,0,i,3:7] * np.array([w,h,w,h])\n",
    "        (startX, startY, endX, endY) = box.astype(int)\n",
    "        print(startX, startY, endX, endY)\n",
    "        # draw the bounding box of face along with label+probability\n",
    "        text = \"{:.2f}%\".format(confidence*100)\n",
    "        y = startY -10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.rectangle(image, (startX, startY), (endX, endY),(0,0,255),2)\n",
    "        cv2.putText(image, text, (startX,y),cv2.FONT_HERSHEY_SIMPLEX,0.45,(0,0,255),2)\n",
    "\n",
    "# displat the final image\n",
    "cv2.imshow(\"Face detected\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Face detection on videos / webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import imutils\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input arguments\n",
    "# path to Caffe deploy prototxt file\n",
    "arg_prototxt = \".//MODELS//FaceDetection//deploy.prototxt\"\n",
    "# path to Caffe pre-trained model\n",
    "arg_model = \".//MODELS//FaceDetection//res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "# minimum probability to filter weak detections\n",
    "arg_confidence = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading face detection model...\n",
      "[INFO] starting video stream...\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "print(\"[INFO] loading face detection model...\")\n",
    "net = cv2.dnn.readNetFromCaffe(arg_prototxt, arg_model)\n",
    "# initialize the video stream\n",
    "print(\"[INFO] starting video stream...\")\n",
    "# use below for webcam\n",
    "vs = VideoStream(src=0).start()\n",
    "# for Raspberry Pi + picamera use below\n",
    "#vs = VideoStream(usePiCamera=True).start()\n",
    "# for video file swap out the VideoStream calss for FileVideoStream\n",
    "\n",
    "# allow camera sensor to warm up for 2sec\n",
    "time.sleep(2.0)\n",
    "\n",
    "# loop over the frames and compute face detections\n",
    "while True:\n",
    "    # grab the frame from the threaded video stream and resize it \n",
    "    # to have a maximum width of 400 pixels\n",
    "    frame = vs.read()\n",
    "    #frame = imutils.resize(frame, width=600)\n",
    "    frame = imutils.resize(frame, width=750, height=500)\n",
    "    # grab the frame dimension and conver it to blob\n",
    "    (h,w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage( cv2.resize(frame,(300,300)), 1.0,\n",
    "                                (300,300),(104.0,177.0,123.0))\n",
    "    \n",
    "    # pass the blob thru the network and obtain \n",
    "    # the detection and predictions\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    # detection has the same structure as describe above\n",
    "    # while working with images\n",
    "    \n",
    "    # loop over the detection\n",
    "    for i in np.arange(0, detections.shape[2]):\n",
    "        # extract the confidence\n",
    "        confidence = detections[0,0,i,2]\n",
    "        # filter out the weak detections as per minimum confidence\n",
    "        if confidence < arg_confidence:\n",
    "            continue\n",
    "        \n",
    "        # compute the (x,y) cordinates of BB for the face detected\n",
    "        box = detections[0,0,i,3:7] * np.array([w,h,w,h])\n",
    "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "        # draw the BB of the face along with the associated probability\n",
    "        text = \"{:.2f}%\".format(confidence*100)\n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.rectangle(frame, (startX, startY), (endX, endY), \n",
    "                     (0,0,255),2)\n",
    "        cv2.putText(frame, text, (startX,y), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                   0.45,(0,0,255),2)\n",
    "        \n",
    "    # show the output frame\n",
    "    cv2.imshow(\"Video-Frame\",frame)\n",
    "    #key = cv2.waitKey(1) & 0xFF\n",
    "    # if the \"q\" key was pressed, break from the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "        \n",
    "# release handle to the webcam and clean up\n",
    "vs.stop()\n",
    "cv2.destroyAllWindows()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
