{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face recognition\n",
    "\n",
    "Face recognition using computer vision, openCV, python\n",
    "\n",
    "This will be performed in 2 steps. In step1, we will generate 128-d facial embedding of known faces. In step2, we will check new face embeddings with stored one to identify faces. \n",
    "\n",
    "These 2 steps should be executed as different scripts. Step-1 of generating and storing 128-d facial embeding is one time process whereas step-2 of comparing new face's embedding would be more frequent. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Generate 128-d facial embeddings for known faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import dlib\n",
    "import face_recognition\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from imutils import paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input arguments\n",
    "# path to input directory of faces + images\n",
    "arg_dataset = \".\\\\DATA\\\\face_recognition\\\\dataset\\\\\"\n",
    "# path to serialized db of facial encoding\n",
    "arg_encoding = \".\\\\DATA\\\\face_recognition\\\\encodings.pickle\"\n",
    "# face detection model to use : either hog or cnn\n",
    "# CNN method is more accurate but slower. \n",
    "# HOG method is faster but less accurate\n",
    "arg_detection_method = \"cnn\"\n",
    "# better to use HOG method if running these without GPU (means running on CPU)\n",
    "# because CNN uses more time. \n",
    "# If running on Raspberry Pi, then use HOG because it wont have enough memory to run the CNN. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] quantifying faces...\n"
     ]
    }
   ],
   "source": [
    "# grab the paths to the input images in dataset\n",
    "print(\"[INFO] quantifying faces...\")\n",
    "imagePaths = list(paths.list_images(arg_dataset))\n",
    "\n",
    "# initialize the list of known face encodings and known names\n",
    "knownEncodings = []\n",
    "# list of correspondng known names of each known face encodings\n",
    "knownNames = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing to note here is that OpenCV orders color channels in BGR, but the dlib actually expects RGB. The face_recognition module uses dlib, so before we procees, We need to swap color space. \n",
    "\n",
    "For each iteration of the loop, we are going to detect a face (or possibly multiple faces)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] DONE...\n",
      "[INFO]: Generating face encodings took 85.449 seconds\n",
      "[INFO] serializing encodings...\n"
     ]
    }
   ],
   "source": [
    "# capture start time\n",
    "start = time.time()\n",
    "\n",
    "# loop over the image paths \n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    # extract the person name from the image path\n",
    "    #print(\"[INFO] processing image {}/{}\".format(i+1, len(imagePaths)))\n",
    "    #print(\"[INFO] imagePath:\",imagePath)\n",
    "    name = imagePath.split(os.path.sep)[-2]\n",
    "    #print(\"[INFO] name:\", name) \n",
    "    # load the input image and convert it from BGR (OpenCV ordering)\n",
    "    # to dlib ordering (RGB) \n",
    "    image = cv2.imread(imagePath)\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # NOTE: this is time consuming process\n",
    "    # detect the (x,y)-coordinates of the bounding boxes\n",
    "    # corresponding to each face in the input image\n",
    "    # CNN is more accurate but slower\n",
    "    # HOG is faster but less accurate\n",
    "    boxes = face_recognition.face_locations(rgb, model = arg_detection_method)\n",
    "    #print(\"[INFO] boxes: \",boxes) \n",
    "    \n",
    "    # compute the facial embedding for the face\n",
    "    # Facial encoding is a 128 number array\n",
    "    # this is known as encoding the face into a vector \n",
    "    encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "    #print(\"[INFO] encodings: \",encodings) \n",
    "    # loop over the encodings keep them in list \n",
    "    for encoding in encodings:\n",
    "        # add each encoding + name to our set of known names and encodings\n",
    "        knownEncodings.append(encoding) \n",
    "        knownNames.append(name) \n",
    "\n",
    "print(\"[INFO] DONE...\")\n",
    "# capture end time\n",
    "end = time.time()\n",
    "print(\"[INFO]: Generating face encodings took {:.5} seconds\".format(end - start))\n",
    "\n",
    "# Construct a dictionary with encodings and names and store them as pickle file\n",
    "# store the facial encodings and names to disk\n",
    "print(\"[INFO] serializing encodings...\")\n",
    "data = {\"encodings\":knownEncodings, \"names\":knownNames}\n",
    "f = open(arg_encoding, \"wb\")\n",
    "f.write(pickle.dumps(data))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: For new faces, check embedding with stored one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created our 128-d face embeddings for each image in our dataset, we are now ready to recognize faces in image using OpenCV, python and deep learning. \n",
    "\n",
    "Important thing to note is that CNN algorithm takes time but is more accurate. Whereas HOG is less accurate but takes less time. On CPU, use HOG whereas on GPU use HOG. If GPU not available then another work around could be to generate face-encoding using CNN (which will obviously take time) and run face recognition on new images using HOG which would be fast. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import pickle\n",
    "import cv2\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input arguments\n",
    "\n",
    "# path to serialized db of facial encoding\n",
    "arg_encoding = \".\\\\DATA\\\\face_recognition\\\\encodings.pickle\"\n",
    "# path to input image\n",
    "arg_image = \".\\\\DATA\\\\face_recognition\\\\examples\\\\example_03.png\"\n",
    "# face detection model to use : either hog or cnn\n",
    "#arg_detection_method = \"cnn\"\n",
    "arg_detection_method = \"hog\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pickle encodings of known faces. Any new faces would be checked against these encoding to find out who's face it is. \n",
    "\n",
    "For a new face image, we compute 128-d encodings and initialize a list of names for each face that is detected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading encodings...\n",
      "[INFO] recognizing faces...\n",
      "[INFO]: Generating face encodings took 1.5197 seconds\n"
     ]
    }
   ],
   "source": [
    "# load the known faces and embeddings \n",
    "print(\"[INFO] loading encodings...\")\n",
    "data = pickle.loads(open(arg_encoding,\"rb\").read())\n",
    "\n",
    "# load the input image and convert it from BGR to RGB\n",
    "image = cv2.imread(arg_image)\n",
    "rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# capture start time\n",
    "start = time.time()\n",
    "# detect the (x,y)-coordinates of the bounding box corresponding\n",
    "# to each face in the input image, then compute the facial embeddings\n",
    "# for each face\n",
    "print(\"[INFO] recognizing faces...\")\n",
    "boxes = face_recognition.face_locations(rgb, model = arg_detection_method)\n",
    "encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "# capture end time\n",
    "end = time.time()\n",
    "print(\"[INFO]: Generating face encodings took {:.5} seconds\".format(end - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will attempt to match each face in input image to our known encodings dataset. The function face_recognition.compare_faces returns True/False values, one for each image in our dataset. In this case, our Jurassic Park example, there are 218 images in the dataset and therefore the returned list will have 218 boolean values. \n",
    "\n",
    "Internally, compare_faces function is computing the Euclidean distance between the candidate embedding and all faces in our dataset. \n",
    "- If the distance is below some tolerance (the smaller the tolerance, the more strict our facial recognition system will be) then function returns True, indicating the faces match\n",
    "- Otherwise, if the distance is above the tolerance threshold, function will return False as the faces do not match\n",
    "\n",
    "Essentially, here we are utilizing a \"more fancy\" KNN model for classification. variable name will eventually hold the name string of the person. \n",
    "\n",
    "From the matches list, we can compute the number of \"votes\" for each name (number of True values asociated with each name), tallly up the votes and select the person's name with most corresponding votes;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] matchedIdxs: [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75]\n",
      "[INFO] counts: {'ian_malcolm': 40}\n",
      "[INFO] matchedIdxs: [4, 24, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 112, 139, 150, 159]\n",
      "[INFO] counts: {'john_hammond': 2, 'ellie_sattler': 30, 'claire_dearing': 4}\n",
      "[INFO] matchedIdxs: [162, 164, 167, 168, 169, 170, 171, 172, 174, 175, 176, 177, 178, 180, 181]\n",
      "[INFO] counts: {'alan_grant': 15}\n"
     ]
    }
   ],
   "source": [
    "# initialize the list of names for each face detected\n",
    "names = []\n",
    "\n",
    "# loop over the facial embeddings\n",
    "for encoding in encodings:\n",
    "    # attempt to match each face in the input image to our known encoding \n",
    "    matches = face_recognition.compare_faces(data[\"encodings\"], encoding)\n",
    "    #print(\"[INFO] matches: \",matches)\n",
    "    name = \"Unknown\" \n",
    "    # check to see if we have found a match\n",
    "    if True in matches:\n",
    "        # find the indexes of all matched faces then initialize a \n",
    "        # distionary to count the total number of times each \n",
    "        # face was matched\n",
    "        matchedIdxs = [i for (i,x) in enumerate(matches) if x]\n",
    "        counts = {}\n",
    "        print(\"[INFO] matchedIdxs:\",matchedIdxs)\n",
    "        # loop over the matched indexes and \n",
    "        # maintain a count for each recognized face\n",
    "        for i in matchedIdxs:\n",
    "            name = data[\"names\"][i] \n",
    "            counts[name] = counts.get(name, 0) + 1\n",
    "        \n",
    "        # determine the recognized face with the largest number\n",
    "        # of votes (note: in the event of an unlikely tie, Python\n",
    "        # will select first entry in the dictionary)\n",
    "        print(\"[INFO] counts:\",counts)\n",
    "        name = max(counts, key=counts.get)\n",
    "        #print(\"[INFO] name:\",name)\n",
    "    \n",
    "    # update the list of names\n",
    "    names.append(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For first picture, we got 40 votes for Ian Malcolm which is really good score, given that we have only 41 pictures of Ian. \n",
    "\n",
    "For second face, we got only 5 votes for Alan Grant. But still, there is only one name in the dictionary so we likely have found Alan. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw bounding-box and labeled names for each person and draw them on our output image for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the recognized faces\n",
    "for ((top, right, bottom, left),name) in zip(boxes, names):\n",
    "    # draw the predicted face name on the image\n",
    "    cv2.rectangle(image, (left,top),(right,bottom), (0,255,0),2)\n",
    "    y = top-15 if top-15 > 15 else top + 15\n",
    "    cv2.putText(image, name, (left,y), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               0.75, (0,255,0),2)\n",
    "\n",
    "# show the output image\n",
    "cv2.imshow(\"Recognized faces\",image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognizing faces in videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important Performance Note: \n",
    "The CNN face recognizer should only be used in real-time if you are working with a GPU (you can use it with a CPU, but expect less than 0.5 FPS which makes for a choppy video). Alternatively (you are using a CPU), you should use the HoG method (or even OpenCV Haar cascades covered in a future blog post) and expect adequate speeds.\n",
    "\n",
    "If GPU not available then another work around could be to generate face-encoding using CNN (which will obviously take time) and run face recognition on new images using HOG which would be fast. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import VideoStream\n",
    "import face_recognition\n",
    "import imutils\n",
    "import pickle\n",
    "import time\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse input argument\n",
    "# path to serialized db of facial encodings\n",
    "arg_encodings = \".\\\\DATA\\\\face_recognition\\\\encodings.pickle\"\n",
    "# path to input video\n",
    "arg_input_video = \".\\\\DATA\\\\face_recognition\\\\videos\\\\lunch_scene.mp4\"\n",
    "# path to output video\n",
    "arg_output = \".\\\\DATA\\\\face_recognition\\\\output\\\\input_face_output.avi\"\n",
    "#arg_output = \".\\\\DATA\\\\face_recognition\\\\output\\\\webcam_face_output.avi\"\n",
    "# Whether or not to display output frame to screen\n",
    "arg_display = 1\n",
    "# face detection model to use : either hog or cnn\n",
    "#arg_detection_method = \"cnn\"\n",
    "arg_detection_method = \"hog\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using videostream to access our camera. if you have multiple cameras on your system (such as built-in webcam and an external USB cam), you can change the src=0 to src=1 and so forth. \n",
    "\n",
    "We will be optionally writing processed video frames to disk later, so we initialize write to None. Sleeping for 2 complete seconds allows our camera to warm up. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading encodings...\n",
      "[INFO] starting Input video stream...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'cv2.VideoCapture' object has no attribute 'stop'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-45c9a8fd612a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;31m# do cleanup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m \u001b[0mvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;31m# check to see if the video writer point needs to be released\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mwriter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'cv2.VideoCapture' object has no attribute 'stop'"
     ]
    }
   ],
   "source": [
    "# load the known faces and embeddings\n",
    "print(\"[INFO] loading encodings...\")\n",
    "data = pickle.loads(open(arg_encodings,\"rb\").read())\n",
    "\n",
    "# initialize the video stream and pointer to output video files,\n",
    "# then allow the camera sensor to warm up\n",
    "\n",
    "if not arg_input_video:\n",
    "    print(\"[INFO] starting Webcam stream...\")\n",
    "    vs = VideoStream(src=0).start()\n",
    "else:\n",
    "    print(\"[INFO] starting Input video stream...\")\n",
    "    vs = cv2.VideoCapture(arg_input_video)\n",
    "\n",
    "writer = None\n",
    "time.sleep(2.0)\n",
    "\n",
    "# loop over frames from the video file stream\n",
    "while True:\n",
    "    # grab the frame from the threaded video stream\n",
    "    frame = vs.read()\n",
    "    frame = frame[1] if arg_input_video else frame\n",
    "    \n",
    "    # check to see if we have reached the end of the stream\n",
    "    if frame is None:\n",
    "        break\n",
    "    \n",
    "    # convert the input frame from BGR to RGB then resize it to have \n",
    "    # a width of 750px (to speedup processing)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    rgb = imutils.resize(frame, width=750)\n",
    "    r = frame.shape[1] / float(rgb.shape[1])\n",
    "    \n",
    "    # detect the (x,y)-coordinates of the bounding-boxes\n",
    "    # corresponding to each face in the input frame, then\n",
    "    # compute the facial embeddings for each face\n",
    "    boxes = face_recognition.face_locations(rgb, model=arg_detection_method)\n",
    "    encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "    names = []\n",
    "    # loop over the facial embeddings\n",
    "    for encoding in encodings:\n",
    "        # attempt to match each face in the input image \n",
    "        # to our known encodings\n",
    "        matches = face_recognition.compare_faces(data[\"encodings\"],encoding)\n",
    "        name = \"Unknown\" \n",
    "        \n",
    "        # check to see if we have found a match\n",
    "        if True in matches:\n",
    "            # find the index of all matched faces then initialize a \n",
    "            # dict to count the total number of ties each face\n",
    "            # was matched\n",
    "            matchedIdxs = [i for (i,b) in enumerate(matches) if b]\n",
    "            counts = {}\n",
    "            \n",
    "            # loop over the matched indexes and maintain a count for\n",
    "            # each recognized face\n",
    "            for i in matchedIdxs:\n",
    "                name = data[\"names\"][i]\n",
    "                counts[name] = counts.get(name,0)+1\n",
    "            # determine the recognized face with the largest number of votes\n",
    "            name = max(counts, key=counts.get)\n",
    "        \n",
    "        # update the list of names\n",
    "        names.append(name)\n",
    "        \n",
    "    # loop over the recognized faces\n",
    "    for ((top,right,bottom,left),name) in zip(boxes,name):\n",
    "        # rescale the face coordinates\n",
    "        top = int(top*r)\n",
    "        right = int(right*r)\n",
    "        bottom = int(bottom*r)\n",
    "        left = int(left*r)\n",
    "        \n",
    "        # draw the predicted face names on the image\n",
    "        cv2.rectangle(frame, (left,top),(right,bottom),(0,255,0),2)\n",
    "        y = top - 15 if top - 15 > 15 else top + 15\n",
    "        cv2.putText(frame, name, (left,y),cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                   0.75, (0,255,0),2)\n",
    "    # if the video write is None and we are supposed to write \n",
    "    # the output video to disk, then initialize the writer\n",
    "    if writer is None and arg_output is not None:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "        writer = cv2.VideoWriter(arg_output,fourcc,20,\n",
    "                                (frame.shape[1],frame.shape[0]), True)\n",
    "    # if the write is not None, write the frame with recognized\n",
    "    # faces to disk\n",
    "    if writer is not None:\n",
    "        writer.write(frame)\n",
    "    # check if we are supposed to display the output frame \n",
    "    # to the screen\n",
    "    if arg_display > 0:\n",
    "        cv2.imshow(\"Frame\",frame)\n",
    "        # if the q key was pressed, break from the loop\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    \n",
    "# do cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()\n",
    "# check to see if the video writer point needs to be released\n",
    "if writer is not None:\n",
    "    writer.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
